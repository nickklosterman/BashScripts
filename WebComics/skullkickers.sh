#!/bin/bash

fileExtension=jpg
#this version improves on the previous version in that it doesn't keep respawning feh and you can navigate forward and backward
function performCleanup () {
    echo "Remove temporary files (y/n)?"
    read inputkey
    if [ "$inputkey" == "y" ] || [ "$inputkey" == "Y" ]
    then 
	echo "Removing temporary files"
	rm /tmp/skullkickers*
    else 
	echo "Not removing temporary files"
    fi
}

function performExecOnFile () {
    while read LINE
    do
	echo $LINE
	exec $LINE & #without this backgroudn we only get one image.
	sleep 0.20     # slow the http requests down a bit so we don't get blocked from the server
    done < ${1}
}


function performWgetOnFile () {
    while read LINE
    do
	#	    echo $LINE
	wget $LINE -nv -O - >> /tmp/skullkickers & #without this backgroudn we only get one image.
	sleep 0.20     # slow the http requests down a bit so we don't get blocked from the server
    done < ${1}
}



function fileCheck () {
    for cnter in {1..481}
    do
	filename=$(printf "%03d" ${cnter} );
	if [ ! -e ${filename}.${fileExtension} ] || [ ! -e ${cnter}.${fileExtension} ]
	then
	    echo ${filename} ${cnter}
	fi
    done
    echo "288 failing is a misnomer since there is a 288new.${fileExtension}"

}

function zeroPadNumericFileNames () {
    for cntr in {1..99}
    do 
	filename=$(printf "%03d" ${cntr} );
	echo $filename 
	if [ ! -e ${filename}.${fileExtension} ]
	then 
	    mv ${cntr}.${fileExtension} ${filename}.${fileExtension}
	    #to easily get rid of files run the commands below
	    #rm [1-9][0-9].${fileExtension}
	    #rm [1-9].${fileExtension}
	else
	    echo "not moving ${cntr}.${fileExtension} to ${filename}.${fileExtension} bc it all ready exists"  
	fi
    done
}

function getHTMLPages () {
    echo "getHTMLPages"
    counter=${1}
    while [ $counter -gt 0 ]; do
#http://comic.skullkickers.com/comics/1395403349-sk20140321.jpg
#http://comic.skullkickers.com/index.php?id=494
	echo "http://comic.skullkickers.com/index.php?id=${counter}/ " >> /tmp/skullkickerspages
	#echo ${counter}
	let "counter-=1"
    done

    #	getPagesInFile /tmp/skullkickerspages
    performWgetOnFile /tmp/skullkickerspages
}

function getPagesInFile() {
    echo "getPagesInFile"
    numberOfParallelOperations=3 #you may need to lower this number if two files writing to our temp file collide and one write op is cut short 
    cat $1 | xargs -P $numberOfParallelOperations -r -n 1 wget -nv -O - >> /tmp/skullkickers
}

function renameFilesToSequentialNumbers() {
    #the image names don't easily translate into the correct order
    #luckily there is a nav bar that lets you go to any page.
    #it is autogenerated and has the title of each page in it.
    #this title corresponds to the alt= naming of the image. 
    #now I just need to map the renaming. 
    grep "<img src=" /tmp/skullkickers | sed 's/.*img src=//;s/alt=/,/;s/\/>//;s/ ,/,/' > /tmp/skullkickerscomicimageurlsPart1


    grep "option value" /tmp/skullkickers | sed 's/ selected="selected"//;s/option value="/\^/g;s/<\/option></"/g;s/">/,"/g'| tr '^' '\n' |  sort | uniq  >  /tmp/skullkickerscomicimageurlsPart2

    sort -k2 -t, /tmp/skullkickerscomicimageurlsPart1 | grep "${fileExtension}"  > /tmp/part1.txt
    sort -k2 -t, /tmp/skullkickerscomicimageurlsPart2 | grep "Skullkickers" | sed 's/"".*/"/' > /tmp/part2.txt
    join -j2 -t ',' <(sed 's/\s\+$//'  /tmp/part1.txt) /tmp/part2.txt > /tmp/renamemap.txt #according to my SO post's answer it was the trailing whitespace screwing stuff up. 

    echo "read comments in file"
    read somekey
    #I tried to join these files using the 'join' command but didn't have any luck
    #as it was there were a couple fields missing from one of the files that other had :(
    #I therefore went in and removed the missing entries
    #I then opened libreoffice spreadsheet and merged so that I had a renaming map. 
    #field 1 was the original filename of the ${fileExtension} from the website, fields 2,3 were the matching alt names, field 4 was the number
    #I named the file combined.csv
    #cut -f1,4 -d, combined.csv > cutcombined.csv

    #I then needed to remove the url and just get the txt and insert .${fileExtension} at the end, add my 'mv' command and remove the csv comma
    sed 's/.*http:\/\/www.skullkickerscats.com\/comic\//mv /;s/$/.${fileExtension}/;s/",/ /' /tmp/renamemap.txt > /tmp/renaming.txt
    #bash /tmp/renaming.txt
}

##########
##########
##-Main-##
##########
##########

NumberOfExpectedArguments=1
NumberOfImages=${1}
debugswitch=1 #setting to something besides 1 will turn off bits of code
if [ 1 -eq $debugswitch ]
then 
    if [ $# -ne $NumberOfExpectedArguments ]
    then
	echo "Please specify the number of comics to grab"
	#	echo $#
    else 
echo "I couldn't get this to work"
read somekey
	performCleanup
	
	getHTMLPages $NumberOfImages

	# wait for /tmp/skullkickers to be populated
	sleep 5s	


#src="comics/../comics/1394797286-sk20140314.jpg" 
#http://comic.skullkickers.com/comics/1394797286-sk20140314.jpg
#	grep "src=\"comics/../comics/"  /tmp/skullkickers |  sed 's/.*src="comics\/..\/comics\/http:\/\/comic.skullkickers.com\/comics\//;s/".*//' | sort | uniq  > /tmp/skullkickerscomicimages
	grep'src="comics/'  /tmp/skullkickers  | sort | uniq  > /tmp/skullkickerscomicimages


	#renameFiles

	performExecOnFile /tmp/skullkickerscomicimages

	#wait for all downloads to complete. vary this depending on the size of the images and download speeds.
	sleep 5s

#	zip Skullkickers *.${fileExtension}
#	mv Skullkickers.zip Skullkickers.cbz
#	evince Skullkickers.cbz  &
    fi 
fi


